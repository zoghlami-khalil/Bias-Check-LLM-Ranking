{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e507cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self-preference (self-win) rates over all prompts:\n",
      "                   openai/gpt-4o: 29.87%\n",
      "     anthropic/claude-3.5-sonnet: 42.31%\n",
      "          deepseek/deepseek-chat: 43.13%\n",
      "                perplexity/sonar: 21.63%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\zoghla\\Desktop\\Yosr Capstone\\LLM-Ensemble-Model-main\\Full Dataset Ranking\\ranked_responses_final.csv')\n",
    "\n",
    "\n",
    "# 2) Map each “judge” to the column where it scored responses\n",
    "judges = {\n",
    "    'openai/gpt-4o':                'openai/gpt-4o',\n",
    "    'anthropic/claude-3.5-sonnet':  'anthropic/claude-3.5-sonnet',\n",
    "    'deepseek/deepseek-chat':       'deepseek/deepseek-chat',\n",
    "    'perplexity/sonar':             'perplexity/sonar'\n",
    "}\n",
    "\n",
    "# 3) Compute, for each judge, how often it ranks its *own* response highest\n",
    "self_win_rates = {}\n",
    "for judge_model, score_col in judges.items():\n",
    "    self_wins = 0\n",
    "    total    = 0\n",
    "    # iterate over all unique prompts\n",
    "    for prompt_id, group in df.groupby('ID'):\n",
    "        # find the response with the maximum score *according to* this judge\n",
    "        idx_max = group[score_col].idxmax()\n",
    "        winner  = df.at[idx_max, 'Model']\n",
    "        total  += 1\n",
    "        if winner == judge_model:\n",
    "            self_wins += 1\n",
    "    self_win_rates[judge_model] = self_wins / total * 100.0\n",
    "\n",
    "# 4) Print out the self-preference rates\n",
    "print(\"Self-preference (self-win) rates over all prompts:\")\n",
    "for jm, rate in self_win_rates.items():\n",
    "    print(f\"  {jm:>30s}: {rate:5.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9f2c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unanimous agreement: 1892/5289 = 35.77%\n",
      "\n",
      "Pairwise agreement rates:\n",
      "                   openai/gpt-4o vs anthropic/claude-3.5-sonnet   : 61.69%\n",
      "                   openai/gpt-4o vs deepseek/deepseek-chat        : 61.71%\n",
      "                   openai/gpt-4o vs perplexity/sonar              : 62.60%\n",
      "     anthropic/claude-3.5-sonnet vs deepseek/deepseek-chat        : 62.79%\n",
      "     anthropic/claude-3.5-sonnet vs perplexity/sonar              : 64.06%\n",
      "          deepseek/deepseek-chat vs perplexity/sonar              : 59.50%\n",
      "\n",
      "Distribution of how many judges picked each model as #1:\n",
      "                   openai/gpt-4o  anthropic/claude-3.5-sonnet  deepseek/deepseek-chat  perplexity/sonar\n",
      "num_judges_picked                                                                                      \n",
      "0                           3177                         2517                    2394              3733\n",
      "1                           1049                          754                     833               859\n",
      "2                            438                          451                     567               295\n",
      "3                            389                          762                     793               253\n",
      "4                            236                          805                     702               149\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# 1) Load your full dataset\n",
    "df = pd.read_csv(r'C:\\Users\\zoghla\\Desktop\\Yosr Capstone\\LLM-Ensemble-Model-main\\Full Dataset Ranking\\ranked_responses_final.csv')\n",
    "\n",
    "# 2) Define the four judges and the columns where they scored each response\n",
    "judges = {\n",
    "    'openai/gpt-4o':               'openai/gpt-4o',\n",
    "    'anthropic/claude-3.5-sonnet': 'anthropic/claude-3.5-sonnet',\n",
    "    'deepseek/deepseek-chat':      'deepseek/deepseek-chat',\n",
    "    'perplexity/sonar':            'perplexity/sonar'\n",
    "}\n",
    "\n",
    "# 3) Build a table of “top choice per judge per prompt”\n",
    "#    -> top_choices[judge] is a Series indexed by ID with the Model they ranked highest\n",
    "top_choices = {}\n",
    "for judge_name, score_col in judges.items():\n",
    "    top = (df\n",
    "           .loc[df.groupby('ID')[score_col].idxmax()]  # for each ID, pick row with max score_col\n",
    "           .set_index('ID')['Model'])\n",
    "    top_choices[judge_name] = top\n",
    "\n",
    "top_df = pd.DataFrame(top_choices)  # shape = (#unique IDs) × 4\n",
    "\n",
    "# 4) Unanimous agreement: all judges picked the same Model\n",
    "n_prompts = len(top_df)\n",
    "n_unanimous = (top_df.nunique(axis=1) == 1).sum()\n",
    "print(f\"Unanimous agreement: {n_unanimous}/{n_prompts} = {n_unanimous/n_prompts*100:.2f}%\\n\")\n",
    "\n",
    "# 5) Pairwise agreement between judges\n",
    "print(\"Pairwise agreement rates:\")\n",
    "for j1, j2 in itertools.combinations(judges.keys(), 2):\n",
    "    agree_rate = (top_df[j1] == top_df[j2]).mean() * 100\n",
    "    print(f\"  {j1:>30s} vs {j2:<30s}: {agree_rate:5.2f}%\")\n",
    "print()\n",
    "\n",
    "# 6) For each model: count how many prompts had exactly k judges pick it (k=1..4)\n",
    "dist = {}\n",
    "for model in judges.keys():\n",
    "    # For each prompt (row), count how many judges picked this model\n",
    "    counts = top_df.eq(model).sum(axis=1).value_counts().sort_index()\n",
    "    # ensure entries for 0–4\n",
    "    dist[model] = [counts.get(k, 0) for k in range(5)]\n",
    "\n",
    "dist_df = pd.DataFrame(dist, index=range(5))\n",
    "dist_df.index.name = 'num_judges_picked'\n",
    "print(\"Distribution of how many judges picked each model as #1:\")\n",
    "print(dist_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28262499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss' kappa: 0.4709\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Path to your CSV file – update this to your actual file location\n",
    "csv_path = r'C:\\Users\\zoghla\\Desktop\\Yosr Capstone\\LLM-Ensemble-Model-main\\Full Dataset Ranking\\ranked_responses_final.csv'\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(csv_path):\n",
    "    print(f\"File not found: {csv_path}. Please update 'csv_path' to your dataset location.\")\n",
    "else:\n",
    "    # Load the data\n",
    "    df = pd.read_csv(csv_path, sep=',')\n",
    "\n",
    "    # Define the judges and their score columns\n",
    "    judges = {\n",
    "        'openai/gpt-4o':               'openai/gpt-4o',\n",
    "        'anthropic/claude-3.5-sonnet': 'anthropic/claude-3.5-sonnet',\n",
    "        'deepseek/deepseek-chat':      'deepseek/deepseek-chat',\n",
    "        'perplexity/sonar':            'perplexity/sonar'\n",
    "    }\n",
    "\n",
    "    # Compute top choice per judge per prompt\n",
    "    top_choices = {\n",
    "        name: df.loc[df.groupby('ID')[col].idxmax()].set_index('ID')['Model']\n",
    "        for name, col in judges.items()\n",
    "    }\n",
    "    top_df = pd.DataFrame(top_choices)\n",
    "\n",
    "    # Build the count matrix n_ij: rows=prompts, cols=models\n",
    "    categories = list(judges.keys())\n",
    "    count_matrix = np.array([\n",
    "        [row.tolist().count(cat) for cat in categories]\n",
    "        for _, row in top_df.iterrows()\n",
    "    ])\n",
    "\n",
    "    # Fleiss' kappa calculation\n",
    "    N, k = count_matrix.shape  # N items, k categories\n",
    "    m = 4  # number of judges\n",
    "    # Proportion of all assignments to category j\n",
    "    p = count_matrix.sum(axis=0) / (N * m)\n",
    "    # Agreement for each item\n",
    "    P_i = (np.sum(count_matrix * (count_matrix - 1), axis=1)) / (m * (m - 1))\n",
    "    P_bar = np.mean(P_i)\n",
    "    P_e = np.sum(p ** 2)\n",
    "    kappa = (P_bar - P_e) / (1 - P_e)\n",
    "\n",
    "    print(f\"Fleiss' kappa: {kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5624c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt mapping: 500/500 = 100.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# 1) Load your ranking system results\n",
    "our_df = pd.read_csv('ranked_responses_final.csv')\n",
    "\n",
    "# 2) Extract our system’s top-1 per prompt\n",
    "top_our = (\n",
    "    our_df\n",
    "    .loc[our_df.groupby('Prompt')['Final Score'].idxmax()]\n",
    "    .loc[:, ['Prompt','Model']]\n",
    "    .rename(columns={'Model':'our_best_model'})\n",
    ")\n",
    "\n",
    "# 3) Load Gemini’s rankings\n",
    "gem_df = pd.read_csv('ranked_results_with_explanations.csv')\n",
    "\n",
    "# 3.5) Prompt‐mapping check\n",
    "total_gem = len(gem_df)\n",
    "mapped    = gem_df['Prompt'].isin(our_df['Prompt']).sum()\n",
    "print(f\"Prompt mapping: {mapped}/{total_gem} = {mapped/total_gem*100:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62cbdde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compared on 499 prompts:\n",
      "  Agreement rate: 364/499 = 72.9%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Merge and compute raw agreement\n",
    "comparison = pd.merge(\n",
    "    top_our, \n",
    "    gem_df[['Prompt','best_model']], \n",
    "    on='Prompt', \n",
    "    how='inner'\n",
    ").dropna(subset=['our_best_model','best_model'])\n",
    "\n",
    "n = len(comparison)\n",
    "agree = (comparison['our_best_model'] == comparison['best_model']).sum()\n",
    "print(f\"Compared on {n} prompts:\")\n",
    "print(f\"  Agreement rate: {agree}/{n} = {agree/n*100:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2d545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini best_model counts:\n",
      "best_model\n",
      "deepseek/deepseek-chat         200\n",
      "anthropic/claude-3.5-sonnet    177\n",
      "openai/gpt-4o                   71\n",
      "perplexity/sonar                51\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Our system best_model counts (on 500 sampled prompts):\n",
      "our_best_model\n",
      "deepseek/deepseek-chat         182\n",
      "anthropic/claude-3.5-sonnet    175\n",
      "openai/gpt-4o                   94\n",
      "perplexity/sonar                49\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) Distribution of Gemini’s picks\n",
    "print(\"Gemini best_model counts:\")\n",
    "print(gem_df['best_model'].value_counts(), \"\\n\")\n",
    "\n",
    "# 6) Distribution of our system’s picks on the *same* sampled prompts\n",
    "sampled_prompts = gem_df['Prompt'].unique()\n",
    "our_sampled = top_our[top_our['Prompt'].isin(sampled_prompts)]\n",
    "print(f\"Our system best_model counts (on {len(our_sampled)} sampled prompts):\")\n",
    "print(our_sampled['our_best_model'].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1017ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen’s κ: 0.6098\n"
     ]
    }
   ],
   "source": [
    "# 7) Cohen’s κ between the two “raters”\n",
    "kappa = cohen_kappa_score(comparison['our_best_model'], comparison['best_model'])\n",
    "print(f\"Cohen’s κ: {kappa:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
